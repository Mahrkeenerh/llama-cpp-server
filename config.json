{
  "server": {
    "host": "127.0.0.1",
    "port": 8080,
    "cors_origins": ["http://localhost:5000"],
    "log_level": "INFO"
  },
  "model_manager": {
    "models_directory": "/mnt/DataShare/Models/LLM",
    "idle_timeout": 300,
    "check_interval": 60,
    "n_ctx": 8192,
    "n_gpu_layers": -1,
    "n_threads": 8,
    "default_model": "Qwen3-8B-Q8_0.gguf",
    "override_tensor": null,
    "offload_kqv": true
  },
  "model_settings": {
    "Qwen3-14B-Q6_K": {
      "n_ctx": 8192
    },
    "Qwen3-14B-Q4_K_M": {
      "n_ctx": 16384
    },
    "Qwen3-30B-A3B-Q6_K": {
      "n_ctx": 16384,
      "override_tensor": ".ffn_.*_exps.=CPU"
    },
    "gpt-oss-20b-Q8_0": {
      "n_ctx": 12288
    },
    "gpt-oss-20b-Q8_0-128k": {
      "file": "gpt-oss-20b-Q8_0.gguf",
      "n_ctx": 131072,
      "override_tensor": ".ffn_.*_exps.=CPU"
    },
    "Phi-3.5-MoE-instruct-Q6_K_L": {
      "n_ctx": 16384,
      "override_tensor": ".ffn_.*_exps.=CPU"
    },
    "Phi-3.5-MoE-instruct-Q8_0": {
      "n_ctx": 16384,
      "override_tensor": ".ffn_.*_exps.=CPU"
    }
  },
  "_comments": {
    "override_tensor": "For MoE models, use: .ffn_.*_exps.=CPU to offload experts to CPU RAM",
    "offload_kqv": "Set to false to keep KV cache in CPU RAM instead of GPU VRAM"
  }
}
