{
  "name": "llama-cpp-server",
  "version": "1.0.0",
  "description": "OpenAI-compatible LLM inference server using llama.cpp with lazy model loading",
  "type": "user-service",

  "systemd": {
    "unit": "systemd/llama-cpp-server.service",
    "timer": null,
    "service_name": "llama-cpp-server.service"
  },

  "network": {
    "port": 8080,
    "health": "/health"
  },

  "paths": {
    "config": "config.json",
    "logs": "journalctl",
    "install": "install.sh",
    "uninstall": "uninstall.sh"
  },

  "ui": {
    "icon": "ðŸ¤–",
    "category": "AI"
  }
}
